{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd20cc76",
   "metadata": {},
   "source": [
    "# Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8088b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yolo\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7e10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:00<00:00, 8.02MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model = YOLO('yolo11n.pt')  # Load a pretrained YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b802443",
   "metadata": {},
   "source": [
    "#### Perform Predication on an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce75af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\img.jpg: 448x640 10 persons, 1 fire hydrant, 1 laptop, 122.8ms\n",
      "Speed: 5.4ms preprocess, 122.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n",
      "1 label saved to runs\\detect\\predict2\\labels\n"
     ]
    }
   ],
   "source": [
    "img_results = model.predict(source='data/img.jpg', save=True, save_txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results of the image prediction\n",
    "img_results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e805be",
   "metadata": {},
   "source": [
    "#### Perform Predication on an Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ca6da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING \n",
      "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 2 cars, 2 motorcycles, 1 truck, 382.0ms\n",
      "video 1/1 (frame 2/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 6 persons, 2 cars, 163.8ms\n",
      "video 1/1 (frame 3/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 5 persons, 2 cars, 223.1ms\n",
      "video 1/1 (frame 4/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 394.6ms\n",
      "video 1/1 (frame 5/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 284.7ms\n",
      "video 1/1 (frame 6/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 2 cars, 217.5ms\n",
      "video 1/1 (frame 7/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 2 cars, 282.0ms\n",
      "video 1/1 (frame 8/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 191.4ms\n",
      "video 1/1 (frame 9/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 1 motorcycle, 214.2ms\n",
      "video 1/1 (frame 10/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 218.7ms\n",
      "video 1/1 (frame 11/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 1 motorcycle, 265.9ms\n",
      "video 1/1 (frame 12/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 249.6ms\n",
      "video 1/1 (frame 13/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 224.4ms\n",
      "video 1/1 (frame 14/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 216.9ms\n",
      "video 1/1 (frame 15/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 210.0ms\n",
      "video 1/1 (frame 16/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 195.7ms\n",
      "video 1/1 (frame 17/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 193.9ms\n",
      "video 1/1 (frame 18/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 177.7ms\n",
      "video 1/1 (frame 19/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 215.5ms\n",
      "video 1/1 (frame 20/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 164.7ms\n",
      "video 1/1 (frame 21/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 131.5ms\n",
      "video 1/1 (frame 22/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 161.4ms\n",
      "video 1/1 (frame 23/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 213.4ms\n",
      "video 1/1 (frame 24/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 186.8ms\n",
      "video 1/1 (frame 25/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 157.5ms\n",
      "video 1/1 (frame 26/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 196.7ms\n",
      "video 1/1 (frame 27/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 184.6ms\n",
      "video 1/1 (frame 28/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 181.1ms\n",
      "video 1/1 (frame 29/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 1 truck, 180.5ms\n",
      "video 1/1 (frame 30/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 131.3ms\n",
      "video 1/1 (frame 31/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 1 truck, 142.5ms\n",
      "video 1/1 (frame 32/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 115.1ms\n",
      "video 1/1 (frame 33/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 123.3ms\n",
      "video 1/1 (frame 34/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 148.2ms\n",
      "video 1/1 (frame 35/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 137.8ms\n",
      "video 1/1 (frame 36/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 146.2ms\n",
      "video 1/1 (frame 37/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 163.3ms\n",
      "video 1/1 (frame 38/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 124.5ms\n",
      "video 1/1 (frame 39/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 132.3ms\n",
      "video 1/1 (frame 40/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 4 cars, 1 motorcycle, 177.3ms\n",
      "video 1/1 (frame 41/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 1 truck, 133.2ms\n",
      "video 1/1 (frame 42/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 1 handbag, 135.2ms\n",
      "video 1/1 (frame 43/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 1 truck, 1 handbag, 157.3ms\n",
      "video 1/1 (frame 44/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 1 truck, 1 handbag, 161.9ms\n",
      "video 1/1 (frame 45/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 2 motorcycles, 134.9ms\n",
      "video 1/1 (frame 46/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 2 motorcycles, 1 handbag, 117.7ms\n",
      "video 1/1 (frame 47/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 2 motorcycles, 1 truck, 1 handbag, 133.5ms\n",
      "video 1/1 (frame 48/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 3 motorcycles, 142.1ms\n",
      "video 1/1 (frame 49/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 2 motorcycles, 171.6ms\n",
      "video 1/1 (frame 50/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 3 motorcycles, 115.6ms\n",
      "video 1/1 (frame 51/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 3 motorcycles, 131.8ms\n",
      "video 1/1 (frame 52/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 3 motorcycles, 1 handbag, 152.4ms\n",
      "video 1/1 (frame 53/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 123.7ms\n",
      "video 1/1 (frame 54/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 2 motorcycles, 129.0ms\n",
      "video 1/1 (frame 55/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 2 motorcycles, 226.5ms\n",
      "video 1/1 (frame 56/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 104.5ms\n",
      "video 1/1 (frame 57/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 2 motorcycles, 123.1ms\n",
      "video 1/1 (frame 58/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 1 handbag, 104.6ms\n",
      "video 1/1 (frame 59/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 1 handbag, 123.6ms\n",
      "video 1/1 (frame 60/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 127.2ms\n",
      "video 1/1 (frame 61/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 112.6ms\n",
      "video 1/1 (frame 62/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 2 motorcycles, 114.3ms\n",
      "video 1/1 (frame 63/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 5 persons, 1 car, 2 motorcycles, 139.0ms\n",
      "video 1/1 (frame 64/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 2 motorcycles, 1 handbag, 125.9ms\n",
      "video 1/1 (frame 65/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 3 motorcycles, 132.3ms\n",
      "video 1/1 (frame 66/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 2 motorcycles, 110.8ms\n",
      "video 1/1 (frame 67/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 3 motorcycles, 124.6ms\n",
      "video 1/1 (frame 68/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 3 motorcycles, 138.7ms\n",
      "video 1/1 (frame 69/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 3 motorcycles, 119.1ms\n",
      "video 1/1 (frame 70/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 2 motorcycles, 141.7ms\n",
      "video 1/1 (frame 71/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 3 cars, 3 motorcycles, 127.9ms\n",
      "video 1/1 (frame 72/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 cars, 2 motorcycles, 119.8ms\n",
      "video 1/1 (frame 73/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 cars, 2 motorcycles, 112.8ms\n",
      "video 1/1 (frame 74/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 4 cars, 2 motorcycles, 119.8ms\n",
      "video 1/1 (frame 75/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 4 cars, 3 motorcycles, 127.5ms\n",
      "video 1/1 (frame 76/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 3 cars, 2 motorcycles, 142.8ms\n",
      "video 1/1 (frame 77/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 3 cars, 2 motorcycles, 119.9ms\n",
      "video 1/1 (frame 78/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 104.0ms\n",
      "video 1/1 (frame 79/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 2 motorcycles, 105.8ms\n",
      "video 1/1 (frame 80/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 2 motorcycles, 107.3ms\n",
      "video 1/1 (frame 81/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 2 cars, 2 motorcycles, 125.0ms\n",
      "video 1/1 (frame 82/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 3 cars, 1 motorcycle, 107.8ms\n",
      "video 1/1 (frame 83/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 113.8ms\n",
      "video 1/1 (frame 84/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 101.6ms\n",
      "video 1/1 (frame 85/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 5 persons, 1 car, 1 motorcycle, 157.5ms\n",
      "video 1/1 (frame 86/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 119.7ms\n",
      "video 1/1 (frame 87/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 5 persons, 1 car, 1 motorcycle, 119.5ms\n",
      "video 1/1 (frame 88/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 5 persons, 1 car, 1 motorcycle, 130.1ms\n",
      "video 1/1 (frame 89/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 154.3ms\n",
      "video 1/1 (frame 90/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 5 persons, 1 car, 1 motorcycle, 153.9ms\n",
      "video 1/1 (frame 91/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 140.4ms\n",
      "video 1/1 (frame 92/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 133.6ms\n",
      "video 1/1 (frame 93/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 132.9ms\n",
      "video 1/1 (frame 94/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 141.4ms\n",
      "video 1/1 (frame 95/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 142.2ms\n",
      "video 1/1 (frame 96/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 155.3ms\n",
      "video 1/1 (frame 97/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 137.1ms\n",
      "video 1/1 (frame 98/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 138.9ms\n",
      "video 1/1 (frame 99/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 5 persons, 1 car, 1 motorcycle, 163.1ms\n",
      "video 1/1 (frame 100/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 2 motorcycles, 152.8ms\n",
      "video 1/1 (frame 101/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 2 motorcycles, 123.7ms\n",
      "video 1/1 (frame 102/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 2 motorcycles, 141.7ms\n",
      "video 1/1 (frame 103/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 2 motorcycles, 131.0ms\n",
      "video 1/1 (frame 104/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 132.2ms\n",
      "video 1/1 (frame 105/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 160.4ms\n",
      "video 1/1 (frame 106/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 1 motorcycle, 124.7ms\n",
      "video 1/1 (frame 107/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 1 motorcycle, 124.1ms\n",
      "video 1/1 (frame 108/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 122.2ms\n",
      "video 1/1 (frame 109/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 141.3ms\n",
      "video 1/1 (frame 110/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 246.3ms\n",
      "video 1/1 (frame 111/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 194.0ms\n",
      "video 1/1 (frame 112/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 133.8ms\n",
      "video 1/1 (frame 113/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 1 motorcycle, 123.3ms\n",
      "video 1/1 (frame 114/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 2 cars, 2 motorcycles, 152.1ms\n",
      "video 1/1 (frame 115/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 1 motorcycle, 118.3ms\n",
      "video 1/1 (frame 116/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 133.4ms\n",
      "video 1/1 (frame 117/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 1 car, 1 motorcycle, 134.7ms\n",
      "video 1/1 (frame 118/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 1 motorcycle, 126.6ms\n",
      "video 1/1 (frame 119/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 3 cars, 1 motorcycle, 135.0ms\n",
      "video 1/1 (frame 120/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 1 motorcycle, 130.6ms\n",
      "video 1/1 (frame 121/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 2 motorcycles, 145.4ms\n",
      "video 1/1 (frame 122/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 1 motorcycle, 252.8ms\n",
      "video 1/1 (frame 123/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 2 cars, 1 motorcycle, 147.2ms\n",
      "video 1/1 (frame 124/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 1 motorcycle, 140.6ms\n",
      "video 1/1 (frame 125/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 134.5ms\n",
      "video 1/1 (frame 126/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 2 cars, 142.6ms\n",
      "video 1/1 (frame 127/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 146.3ms\n",
      "video 1/1 (frame 128/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 3 cars, 1 motorcycle, 133.8ms\n",
      "video 1/1 (frame 129/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 133.8ms\n",
      "video 1/1 (frame 130/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 148.1ms\n",
      "video 1/1 (frame 131/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 146.4ms\n",
      "video 1/1 (frame 132/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 138.9ms\n",
      "video 1/1 (frame 133/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 135.2ms\n",
      "video 1/1 (frame 134/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 1 motorcycle, 138.8ms\n",
      "video 1/1 (frame 135/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 2 cars, 127.6ms\n",
      "video 1/1 (frame 136/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 176.0ms\n",
      "video 1/1 (frame 137/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 134.2ms\n",
      "video 1/1 (frame 138/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 171.2ms\n",
      "video 1/1 (frame 139/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 139.1ms\n",
      "video 1/1 (frame 140/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 3 cars, 1 motorcycle, 139.7ms\n",
      "video 1/1 (frame 141/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 122.2ms\n",
      "video 1/1 (frame 142/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 120.3ms\n",
      "video 1/1 (frame 143/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 1 car, 1 motorcycle, 111.6ms\n",
      "video 1/1 (frame 144/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 1 motorcycle, 97.2ms\n",
      "video 1/1 (frame 145/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 1781.3ms\n",
      "video 1/1 (frame 146/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 102.9ms\n",
      "video 1/1 (frame 147/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 99.1ms\n",
      "video 1/1 (frame 148/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 134.6ms\n",
      "video 1/1 (frame 149/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 2 cars, 1 motorcycle, 117.1ms\n",
      "video 1/1 (frame 150/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 142.9ms\n",
      "video 1/1 (frame 151/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 182.7ms\n",
      "video 1/1 (frame 152/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 114.2ms\n",
      "video 1/1 (frame 153/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 1 motorcycle, 201.0ms\n",
      "video 1/1 (frame 154/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 3 motorcycles, 141.8ms\n",
      "video 1/1 (frame 155/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 214.9ms\n",
      "video 1/1 (frame 156/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 208.7ms\n",
      "video 1/1 (frame 157/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 3 motorcycles, 161.6ms\n",
      "video 1/1 (frame 158/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 127.8ms\n",
      "video 1/1 (frame 159/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 2 motorcycles, 129.2ms\n",
      "video 1/1 (frame 160/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 3 motorcycles, 138.4ms\n",
      "video 1/1 (frame 161/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 3 motorcycles, 129.0ms\n",
      "video 1/1 (frame 162/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 1 car, 4 motorcycles, 1 umbrella, 192.8ms\n",
      "video 1/1 (frame 163/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 2 motorcycles, 1 umbrella, 118.2ms\n",
      "video 1/1 (frame 164/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 3 motorcycles, 140.1ms\n",
      "video 1/1 (frame 165/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 2 motorcycles, 142.0ms\n",
      "video 1/1 (frame 166/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 2 motorcycles, 111.4ms\n",
      "video 1/1 (frame 167/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 4 persons, 3 motorcycles, 149.4ms\n",
      "video 1/1 (frame 168/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 motorcycles, 106.9ms\n",
      "video 1/1 (frame 169/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 1 car, 3 motorcycles, 140.8ms\n",
      "video 1/1 (frame 170/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 3 persons, 5 motorcycles, 153.7ms\n",
      "video 1/1 (frame 171/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 6 motorcycles, 103.0ms\n",
      "video 1/1 (frame 172/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 6 motorcycles, 105.1ms\n",
      "video 1/1 (frame 173/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 5 motorcycles, 120.2ms\n",
      "video 1/1 (frame 174/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 5 motorcycles, 109.2ms\n",
      "video 1/1 (frame 175/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 1 person, 4 motorcycles, 102.5ms\n",
      "video 1/1 (frame 176/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 motorcycles, 132.9ms\n",
      "video 1/1 (frame 177/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 motorcycles, 101.0ms\n",
      "video 1/1 (frame 178/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 motorcycles, 133.7ms\n",
      "video 1/1 (frame 179/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 motorcycles, 126.2ms\n",
      "video 1/1 (frame 180/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 motorcycles, 136.2ms\n",
      "video 1/1 (frame 181/181) d:\\Programs\\satyam_seth_learnings\\yolo_learning\\Models\\Detection\\data\\video.mp4: 384x640 2 persons, 4 motorcycles, 120.8ms\n",
      "Speed: 5.8ms preprocess, 158.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n",
      "182 labels saved to runs\\detect\\predict2\\labels\n"
     ]
    }
   ],
   "source": [
    "video_results = model.predict(source='data/video.mp4', save=True, save_txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978231a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results of the video prediction\n",
    "video_results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec15cb",
   "metadata": {},
   "source": [
    "#### Perform Predication on WebCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fcaa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "WARNING \n",
      "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 1 bed, 448.8ms\n",
      "0: 480x640 1 bed, 984.6ms\n",
      "0: 480x640 1 person, 1 bed, 390.1ms\n",
      "0: 480x640 2 persons, 1 bed, 519.1ms\n",
      "0: 480x640 1 person, 1 bed, 446.7ms\n",
      "0: 480x640 1 person, 1 bed, 813.8ms\n",
      "0: 480x640 1 person, 1 bed, 1 cell phone, 382.0ms\n",
      "0: 480x640 2 persons, 1 bed, 436.3ms\n",
      "0: 480x640 1 person, 1 bed, 337.8ms\n",
      "0: 480x640 1 person, 1 bed, 531.1ms\n",
      "0: 480x640 1 person, 1 bed, 733.6ms\n",
      "0: 480x640 1 person, 1 bed, 331.6ms\n",
      "0: 480x640 1 person, 1 bed, 543.2ms\n",
      "0: 480x640 1 person, 1 bed, 450.9ms\n",
      "0: 480x640 1 person, 1 bed, 753.1ms\n",
      "0: 480x640 1 person, 1 bed, 440.5ms\n",
      "0: 480x640 1 person, 1 bed, 537.4ms\n",
      "0: 480x640 1 person, 1 bed, 401.4ms\n",
      "0: 480x640 1 person, 1 bottle, 1 bed, 416.4ms\n",
      "0: 480x640 1 person, 1 bottle, 1 bed, 816.0ms\n",
      "0: 480x640 1 person, 1 bed, 316.3ms\n",
      "0: 480x640 1 person, 1 bottle, 1 bed, 492.1ms\n",
      "0: 480x640 1 person, 1 bed, 384.1ms\n",
      "0: 480x640 1 person, 1 bed, 984.3ms\n",
      "0: 480x640 1 person, 1 bed, 1181.7ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m video_results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_txt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:555\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:227\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:57\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m                 response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:641\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    181\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:317\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    316\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    318\u001b[39m     y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:92\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     83\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programs\\satyam_seth_learnings\\yolo_learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "video_results = model.predict(source=0, save=True, save_txt=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
